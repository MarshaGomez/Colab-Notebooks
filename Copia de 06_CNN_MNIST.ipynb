{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copia de 06_CNN_MNIST.ipynb","provenance":[{"file_id":"1hmd6mHN-x_HLtbqXASJUZbg1vLqbG4R-","timestamp":1605864615323},{"file_id":"1yU9kkiNJcnGG0PXom4SodMISiOq2FyO0","timestamp":1603468463564}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"yl9Ncvbv-oLw"},"source":["# Convolutional Neural Networks\n","Images from the MNIST dataset have a fixed size of $28*28$. In the previous notebook we tackled the classification problem with a fully connected feed-forward neural network: we reshaped the input image in order to obtain a 1-D tensor with $28*28 = 784$ elements. The input layer was fed to an hidden layer with 512 units. The number of parameters (weights and biases) is indeed $784*512+512=401920$.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7ZwZMBd3IhCP"},"source":["Now, consider an **high resolution image** with $1280*720$ pixels. A fully connected approach would require a huge number of parameters. Suppose that the first hidden layer is again made up of 512 units, than the number of parameters would be $1280*720*512+512=471,859,712$: *It is almost half a billion of parameters!* In such situation, **the dense, fully connected, approach is practically unfeasible**. Furthermore, after the *reshape* operation, the spatial structure of the input is lost.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Oeb-Xw0-hg_M"},"source":["**Convolutional Neural Network (CNN)** is a particular class of Deep Feed-Forward Neural Networks that overcomes the aforementioned limitations and has proved to be particularly suitable for computer vision applications.\n","There are two main advantages in using CNNs: \n","- thanks to their architecture, they can take into account the spatial structure of the input; this is a desired property when the input neurons are the pixels of an image. \n","- they require fewer parameters than fully-connected networks. This means that they are faster to train, less prone to overfitting, and that deeper and more powerful models can be designed.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"p-jcx1Ptihih"},"source":["## Architecture and Properties\n","These properties originate from the following ideas behind CNN:\n","- Local Receptive fields\n","- Shared weights\n","- Pooling"]},{"cell_type":"markdown","metadata":{"id":"ukUmED0Si8Nv"},"source":["### Local Receptive Fields\n","In a fully-connected network every neuron of a layer is connected to a neuron in the following layer. \n","\n","The expression *local receptive fields* indicates a sparse interaction, where only a limited number of neurons in the $i^{th}$ layer is connected to a neuron in the $(i+1)^{th}$ layer.\n","\n","![local receptive fields](https://miro.medium.com/max/600/1*N7SyP4OvPB8-YpbEUMOK7Q.png)\n"]},{"cell_type":"markdown","metadata":{"id":"npXRMip9jDML"},"source":["### Shared Weights: Convolution Operation\n","In the figure above, the weights used to connect the first receptive field with the neuron in the hidden layer are the same for every connection between receptive fields and corresponding hidden neurons. This is the core of the convolution operation: the first step to obtain the $(i+1)^{th}$ layer is the convolution between the $i^{th}$ layer and a **kernel** (or **filter**). The kernel size corresponds to the size of the receptive field, while the values are the shared parameters. Since this filtering extracts a feature of the $i^{th}$ layer, the $(i+1)^{th}$ layer is often referred to as a feature map.\n","The animated gif below shows the convolution operation between bidimensional input I and kernel K.\n","\n","<img src=\"https://cdn-images-1.medium.com/max/1600/1*VVvdh-BUKFh2pwDD0kPeRA@2x.gif\" width=\"400\"/>\n","\n","- Input I: blue matrix $5*5$\n","- Kernel K: green matrix $3*3$\n","- Feature map: pink matrix $3*3$\n","- Stride (step of the convolution operation) = 1\n","\n","Given an input image of shape $H*W$, a kernel K of shape $KH*KW$, and a stride $S$ , the convolution output has shape: \n","> output width = $\\dfrac{W-KW}{S}+1$\n","\n","> output height = $\\dfrac{H-KH}{S}+1$\n","\n","A **striding** value greater than 1 is tipically used to reduce the computational burden of a convolutional layer.\n","\n","In order to preserve the size of the original image, we can adopt zero **padding**. It consists in adding zeros at the border of the image. It is typically used with very deep architecture to preserve the resolution across many layers. The result of zero padding (P=1) is shown in the following animated figure.  \n","\n","Considering zero padding, the convolution output has shape: \n","> output width = $\\dfrac{W-KW+2P}{S}+1$\n","\n","> output height = $\\dfrac{H-KH+2P}{S}+1$\n","\n","<img src=\"https://cdn-images-1.medium.com/max/1600/1*W2D564Gkad9lj3_6t9I2PA@2x.gif\" width=\"400\"/>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FlTUvZvU2IPB"},"source":["At each convolutional layer, several different feature maps are obtained by using several kernels (or filters). \n","\n","When the network is trained from scratch, the kernels are randomly initialized. The training procedure adjusts kernel weights in a way that allows them to extract significative features from images. \n","\n","![alt text](https://ujwlkarn.files.wordpress.com/2016/08/giphy.gif?w=1000)"]},{"cell_type":"markdown","metadata":{"id":"f7br6KzFjFqU"},"source":["### Pooling\n","A convolutional layer is generally composed by three stages: a linear stage (convolution), a non linear stage (activation function) and a pooling stage.\n","The pooling stage consists in replacing a group of contiguous neurons by one neuron, representing a summary statistic of them. \n","\n","Pooling operation leads to two main consequences:\n","- it reduces the size of a layer: max-pooling with a 2x2 kernel, for example, halves the dimensions of the layer by choosing the maximum values of non overlapping 2x2 windows of neurons. The reduction of number of neurons implies a reduction of the number of connections and, indeed, of parameters.\n","- it guarantees an increased translation invariance because it maps the information of a group of neurons in only one neuron of the next layer; it is more important to know whether a feature is present or not, than its exact location.\n","\n","Typical pooling function are Max-pooling and Average-pooling\n","\n","The figure below shows the application of Max-Pooling operation  using 2x2 non-overlapping kernels\n","![pooling](https://upload.wikimedia.org/wikipedia/commons/e/e9/Max_pooling.png)\n"]},{"cell_type":"markdown","metadata":{"id":"yU_JAqkczAsr"},"source":["## To sum up:\n","The building blocks of a hidden (convolutional-pooling) layer are the following:\n","- Linear Convolutional Stage\n","- Non Linear Activation Stage\n","- Pooling Stage\n","\n","\n","The figure below shows a simple convolutional neural network architecture:\n","\n","![alt text](https://cdn-images-1.medium.com/max/1600/1*N4h1SgwbWNmtrRhszM9EJg.png)\n","\n","A typical CNN architecture consists of an input layer, one or more hidden layers, and an output layer. As shown in figure above, one or more fully connected layers typically elaborate the feature maps extracted by the last convolutional layer. \n"]},{"cell_type":"markdown","metadata":{"id":"YpPdg_Ft_cGP"},"source":["# A simple CNN for MNIST problem\n","- Chapter 5, Section 1 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). \n"]},{"cell_type":"code","metadata":{"id":"9VifYWBYbBoW"},"source":["from tensorflow import keras\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V0kqlp-ibJpL"},"source":["We will use our CNN to classify MNIST digits, a task that you've already been through in previous notebooks, using a fully-connected network. Even though our CNN will be very basic, its \n","accuracy will overcome that of the fully-connected model from previous notebooks.\n"]},{"cell_type":"markdown","metadata":{"id":"5eokTE9LcJM2"},"source":["## Download and prepare the dataset"]},{"cell_type":"markdown","metadata":{"id":"6Bea14sic0Az"},"source":["Importantly, a CNN takes as input tensors of shape `(image_height, image_width, image_channels)` (not including the batch dimension, remind we use *channel-last* convention of TensorFlow backend). \n","In our case, we will configure our CNN to process inputs of size `(28, 28, 1)`, which is the format of MNIST images. We do this via passing the argument `input_shape=(28, 28, 1)` to our first layer."]},{"cell_type":"code","metadata":{"id":"XDAICtkMcjbv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605861821317,"user_tz":-60,"elapsed":1589,"user":{"displayName":"Alessandro Renda","photoUrl":"","userId":"15789039737377305746"}},"outputId":"43a0fc17-3ebf-438d-a124-182d8d792d7d"},"source":["from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.utils import to_categorical\n","\n","(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n","\n","train_images = train_images.reshape((60000, 28, 28, 1))\n","train_images = train_images.astype('float32') / 255\n","\n","test_images = test_images.reshape((10000, 28, 28, 1))\n","test_images = test_images.astype('float32') / 255\n","\n","train_labels = to_categorical(train_labels)\n","test_labels = to_categorical(test_labels)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"29Mho87CgZ2C"},"source":["## Building and training the CNN model"]},{"cell_type":"markdown","metadata":{"id":"4KnPw70gcvqx"},"source":["\n","The backbone of our basic CNN will be a stack of `Conv2D` and `MaxPooling2D` layers. \n","\n","Take a look at the signature of the Conv2D and MaxPooling2D functions. Which arguments must we specify?"]},{"cell_type":"code","metadata":{"id":"8-IlRxfQdf1w"},"source":["keras.layers.Conv2D?"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cBVaeP-KeEFO"},"source":["keras.layers.MaxPooling2D?"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xa_NjaDQQMej"},"source":["from tensorflow.keras import layers\n","from tensorflow.keras import models\n","\n","model = models.Sequential()\n","model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='relu'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"usJh1y4VaDfz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605864789418,"user_tz":-60,"elapsed":1441,"user":{"displayName":"Marsha Gómez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8PXH5itpC_joaNRfQVXhCMGhjNOyFKspnzQpa=s64","userId":"07570468464611825513"}},"outputId":"d6186fdf-7371-4734-8f4c-c773e2e62354"},"source":["# Let's evaluate the number of parameters for the first convolutional layer\n","32*(3*3+1)"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["320"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SrKhiMwT2BFN","executionInfo":{"status":"ok","timestamp":1605864930519,"user_tz":-60,"elapsed":602,"user":{"displayName":"Marsha Gómez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8PXH5itpC_joaNRfQVXhCMGhjNOyFKspnzQpa=s64","userId":"07570468464611825513"}},"outputId":"e695f2ca-b012-4a84-d726-1dc6fc93dd8c"},"source":["3*3*32*64+64"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["18496"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"Fnqo5pDbQMel"},"source":["Let's display the architecture of our convnet so far:"]},{"cell_type":"markdown","metadata":{"id":"F5qd_2H3Bt9z"},"source":["- $I$ = input dimensionality (number of channels / feature maps)\n","- $N$ = output dimensionality (number of feature maps, specified in Conv2D)\n","- $K$ = kernel size \n","\n","nu$I*N*K^2+N$"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yD5Lhhe9AC_S","executionInfo":{"status":"ok","timestamp":1605865139452,"user_tz":-60,"elapsed":1039,"user":{"displayName":"Marsha Gómez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8PXH5itpC_joaNRfQVXhCMGhjNOyFKspnzQpa=s64","userId":"07570468464611825513"}},"outputId":"52eed2f5-55dd-4c68-e7d1-7fb6135eddf8"},"source":["64*3*3*64+64"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["36928"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"JCrEYISUQMem","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605862548693,"user_tz":-60,"elapsed":573,"user":{"displayName":"Alessandro Renda","photoUrl":"","userId":"15789039737377305746"}},"outputId":"147ba50f-591b-4227-fe65-91497b92ad64"},"source":["**model.summary()*"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 26, 26, 32)        320       \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n","=================================================================\n","Total params: 55,744\n","Trainable params: 55,744\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pYYektqSQMeo"},"source":["You can see above that the output of every `Conv2D` and `MaxPooling2D` layer is a 3D tensor of shape `(height, width, channels)`. The width \n","and height dimensions tend to shrink as we go deeper in the network. The number of channels is controlled by the first argument passed to \n","the `Conv2D` layers (e.g. 32 or 64).\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WSjEHHAHanrX"},"source":["The next step would be to feed our last output tensor (of shape `(3, 3, 64)`) into a densely-connected classifier network like those you are \n","already familiar with: a stack of `Dense` layers. These classifiers process vectors, which are 1D, whereas our current output is a 3D tensor. \n","So first, we will have to flatten our 3D outputs to 1D, and then add a few `Dense` layers on top:"]},{"cell_type":"code","metadata":{"id":"eksPl-g4QMep"},"source":["model.add(layers.Flatten())\n","model.add(layers.Dense(64, activation='relu'))\n","model.add(layers.Dense(10, activation='softmax'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uY7h_RNDQMer"},"source":["We are going to do 10-way classification, so we use a final layer with 10 outputs and a softmax activation. Now here's what our network \n","looks like:"]},{"cell_type":"code","metadata":{"id":"YYP4FowhQMes","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605864251758,"user_tz":-60,"elapsed":715,"user":{"displayName":"Alessandro Renda","photoUrl":"","userId":"15789039737377305746"}},"outputId":"956a2f26-8697-4aad-c9f5-887f41a9791f"},"source":["model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 26, 26, 32)        320       \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n","_________________________________________________________________\n","flatten (Flatten)            (None, 576)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 64)                36928     \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 10)                650       \n","=================================================================\n","Total params: 93,322\n","Trainable params: 93,322\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5b-1s8VLQMev"},"source":["As you can see, our `(3, 3, 64)` outputs were flattened into vectors of shape `(576,)`, before going through two `Dense` layers.\n","\n","Now, let's train our convnet on the MNIST digits. We will reuse the code we have already covered in our first example."]},{"cell_type":"code","metadata":{"id":"8WCFoc2mQMex","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605864312638,"user_tz":-60,"elapsed":25435,"user":{"displayName":"Alessandro Renda","photoUrl":"","userId":"15789039737377305746"}},"outputId":"b7b27d2a-7947-489c-f647-ad90e198aee3"},"source":["model.compile(optimizer='rmsprop',\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","model.fit(train_images, train_labels, epochs=5, batch_size=64)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","938/938 [==============================] - 4s 4ms/step - loss: 0.1723 - accuracy: 0.9454\n","Epoch 2/5\n","938/938 [==============================] - 4s 4ms/step - loss: 0.0472 - accuracy: 0.9853\n","Epoch 3/5\n","938/938 [==============================] - 3s 4ms/step - loss: 0.0314 - accuracy: 0.9904\n","Epoch 4/5\n","938/938 [==============================] - 3s 4ms/step - loss: 0.0254 - accuracy: 0.9923\n","Epoch 5/5\n","938/938 [==============================] - 4s 4ms/step - loss: 0.0189 - accuracy: 0.9939\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f804c06dfd0>"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"z0bUMSY0sxs-"},"source":["# Exercise\n","- Select the first 6000 samples of the training set.\n","- Assess the performance of the CNN model on a validation set after varying its hyperparameters (e.g. number of layers / filters, size of kernels for convolution and pooling).\n","You may choose to use a function!\n","\n","> ```python\n","> def build_model(param1,param2, ...):\n",">   model = models.Sequential()\n",">   # [TODO]\n",">   return model\n","> ```\n","\n","\n","\n","- Optional: plot the val_accuracy values against one or two hyperparameters.\n"]},{"cell_type":"markdown","metadata":{"id":"TlXIUNoLGFqB"},"source":["Fit Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":167},"id":"ILUsDok7GHrw","executionInfo":{"status":"error","timestamp":1605866444123,"user_tz":-60,"elapsed":1140,"user":{"displayName":"Marsha Gómez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh8PXH5itpC_joaNRfQVXhCMGhjNOyFKspnzQpa=s64","userId":"07570468464611825513"}},"outputId":"81eb0f70-73fd-4568-82fc-11909f0dbd1e"},"source":["model.fit"],"execution_count":12,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-c74f4efbd94d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}]},{"cell_type":"code","metadata":{"id":"WUc9KynnGBou"},"source":["\n","model.compile(optimizer='rmsprop',\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","model.fit(train_images, train_labels, epochs=5, batch_size=64,validation_split = 0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QRVTu5Jpgjop"},"source":["##  Evaluating the model on the test set"]},{"cell_type":"code","metadata":{"id":"xz7hoxx3QMe1"},"source":["test_loss, test_acc = model.evaluate(test_images, test_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ed25QxllQMe4"},"source":["test_acc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q6At7zZ0QMe7"},"source":["While our densely-connected network from previous notebooks had a test accuracy of ~98%, our basic convnet has a test accuracy of ~99%."]}]}